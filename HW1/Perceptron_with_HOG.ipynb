{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nq4eyYnWmhC"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdDscF9imR4d"
      },
      "source": [
        "# https://ithelp.ithome.com.tw/articles/10218801\n",
        "from google.colab import drive\n",
        "# !cp /content/gdrive/***yourdata*** ./data\n",
        "# !cp -R ./drive/'My Drive'/img2 ./img2  # <is this>\n",
        "# !cp '/content/gdrive'./img2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdkWm8g8x8Lb"
      },
      "source": [
        "def preprocess(filepath, imgf, labelf) :\n",
        "  file = open(filepath, 'r')\n",
        "  for line in file.readlines():\n",
        "    img, label = line.split()\n",
        "    imgf.append(img); labelf.append(label)\n",
        "  file.close()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMwQrPeR9ikc"
      },
      "source": [
        "y = []\n",
        "imgs = []\n",
        "preprocess('/content/img2/tes.txt',imgs,y)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsTVElcm8ma6",
        "outputId": "1f320fc9-96ae-411d-c614-84da38370ef7"
      },
      "source": [
        "y"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', '0', '0', '0', '0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqpSkeip9qJV",
        "outputId": "a6b6a721-49e9-40d5-c025-5eb55d713c1b"
      },
      "source": [
        "imgs"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['img2/a1/n02111277_19.JPEG',\n",
              " 'img2/a1/n02111277_25.JPEG',\n",
              " 'img2/a1/n02111277_32.JPEG',\n",
              " 'img2/a1/n02111277_39.JPEG',\n",
              " 'img2/a1/n02111277_48.JPEG']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGUQUi0I-iUR"
      },
      "source": [
        "# https://stackoverflow.com/questions/28390614/opencv-hogdescripter-python\n",
        "def cv2hog(imgl):\n",
        "  histl = []\n",
        "  winSize = (64,64)\n",
        "  blockSize = (16,16)\n",
        "  blockStride = (8,8)\n",
        "  cellSize = (8,8)\n",
        "  nbins = 9\n",
        "  derivAperture = 1\n",
        "  winSigma = 4.\n",
        "  histogramNormType = 0\n",
        "  L2HysThreshold = 2.0000000000000001e-01\n",
        "  gammaCorrection = 0\n",
        "  nlevels = 64\n",
        "  for ig in imgl:\n",
        "    image = cv2.imread(ig,0)\n",
        "    # plt.imshow(image)\n",
        "    hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,\n",
        "                        histogramNormType,L2HysThreshold,gammaCorrection,nlevels)\n",
        "    hist = hog.compute(image, winStride=(8,8), padding=(8,8), locations=((10,20),))\n",
        "    histl.append(hist)\n",
        "  return(histl)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZxbXCQiD90i",
        "outputId": "e78ede61-6536-4ada-9869-4a07de0e1952"
      },
      "source": [
        "print(imgs)\n",
        "hists = cv2hog(imgs)\n",
        "np.shape(hists[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['img2/a1/n02111277_19.JPEG', 'img2/a1/n02111277_25.JPEG', 'img2/a1/n02111277_32.JPEG', 'img2/a1/n02111277_39.JPEG', 'img2/a1/n02111277_48.JPEG']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1764, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS5qhT4w6Q_o"
      },
      "source": [
        "np.save('tra_hog.npy', hists)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSPIJm9xIang"
      },
      "source": [
        "# https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/common/layers.py\n",
        "# https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/common/functions.py\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # softmaxの出力\n",
        "        self.t = None # 教師データ\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)   # overflow measures : looks like to reduce inf situations\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "def softmax2(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp9zQ1otJLd1",
        "outputId": "d4d049eb-7aae-4176-8c27-b74c6f3eb6a0"
      },
      "source": [
        "x = np.random.normal(size=10)\n",
        "print(x)\n",
        "print(softmax(x))\n",
        "print(softmax2(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.18306798  1.82109363  1.64577483 -0.74906252  0.97404695 -0.95554113\n",
            " -0.09205384 -0.12184826  1.42878427  1.0709846 ]\n",
            "[0.12079858 0.22864035 0.19187256 0.01749638 0.09801333 0.01423232\n",
            " 0.03375075 0.03276    0.15444544 0.10799028]\n",
            "[0.12079858 0.22864035 0.19187256 0.01749638 0.09801333 0.01423232\n",
            " 0.03375075 0.03276    0.15444544 0.10799028]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEIigpdTzCxW",
        "outputId": "3a044332-6c9c-444c-a309-10505d24c94e"
      },
      "source": [
        "x = np.array([1000000000000000000,-2,3,-4,5])\n",
        "print(np.max(x, axis=-1, keepdims=True))\n",
        "print(softmax(x))\n",
        "print(softmax2(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1000000000000000000]\n",
            "[1. 0. 0. 0. 0.]\n",
            "[nan  0.  0.  0.  0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRlhOhMJW11_"
      },
      "source": [
        "# https://github.com/PENGZhaoqing/Hog-feature/blob/master/hog.py\n",
        "# https://github.com/karan6181/Softmax-Classifier/blob/master/softmaxClassifier.py\n",
        "# https://github.com/Vercaca/Perceptron\n",
        "\n",
        "# https://machinelearningmastery.com/softmax-activation-function-with-python/\n",
        "# https://learnopencv.com/histogram-of-oriented-gradients/\n",
        "# https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/\n",
        "# https://openhome.cc/Gossip/CodeData/PythonTutorial/AssertDocTestPy3.html\n",
        "\n",
        "# https://numpy.org/devdocs/user/quickstart.html\n",
        "\n",
        "# read file\n",
        "img = cv2.imread('/content/n02111277_19.JPEG', cv2.IMREAD_COLOR)\n",
        "# plt.imshow(img, cmap=plt.cm.gray)\n",
        "# print(img)\n",
        "# np.shape(img)\n",
        "\n",
        "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
        "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
        "plt.show()\n",
        "\n",
        "# hog = Hog_descriptor(img, cell_size=8, bin_size=8)\n",
        "# vector, image = hog.extract()\n",
        "# plt.imshow(image, cmap=plt.cm.gray)\n",
        "# plt.show()\n",
        "\n",
        "# Perceptron (Plot ?)\n",
        "\n",
        "# Activation Function : Softmax\n",
        "# https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0\n",
        "z = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])\n",
        "print(np.exp(z)/sum(np.exp(z)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "pdm0bTm0XMjw",
        "outputId": "08dbdb1e-0926-4442-cd15-d7facfb6e404"
      },
      "source": [
        "# https://stackoverflow.com/questions/28390614/opencv-hogdescripter-python\n",
        "# https://docs.opencv.org/4.5.1/d5/d33/structcv_1_1HOGDescriptor.html#a5c8e8ce0578512fe80493ed3ed88ca83\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<HOGDescriptor 0x7f968645ef10>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#compute(img[, winStride[, padding[, locations]]]) -> descriptors\\nwinStride = (8,8)\\npadding = (8,8)\\nlocations = ((10,20),)\\nhist2 = hog2.compute(image,winStride,padding,locations)\\nnp.shape(hist2)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haMARTgzbbth",
        "outputId": "8feb8221-1ee9-4fa4-b1b4-00f70b04c9ab"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "# drive.mount('/content/gdrive')\n",
        "!cp '/content/gdrive/My Drive/img2/'<img2>\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 0: `cp '/content/gdrive/My Drive/img2/'<img2>'\n",
            " gdrive\t\t     sample_data    'train (1).txt'\n",
            " n02111277_19.JPEG  'test (1).txt'  'val (1).txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "IrPMtSzVv0tg",
        "outputId": "c7df5a98-7718-4ada-a849-ca546ac0b37e"
      },
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/imagesf.zip /content/imagesf\n",
        "files.download(\"/content/imagesf.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: content/imagesf/ (stored 0%)\n",
            "updating: content/imagesf/a1/ (stored 0%)\n",
            "updating: content/imagesf/a1/n02111277_25.JPEG (deflated 1%)\n",
            "updating: content/imagesf/a1/n02111277_39.JPEG (deflated 1%)\n",
            "updating: content/imagesf/a1/n02111277_19.JPEG (deflated 1%)\n",
            "updating: content/imagesf/a1/n02111277_48.JPEG (deflated 1%)\n",
            "updating: content/imagesf/a1/n02111277_32.JPEG (deflated 2%)\n",
            "updating: content/imagesf/a2/ (stored 0%)\n",
            "updating: content/imagesf/a2/n02111277_115.JPEG (deflated 1%)\n",
            "updating: content/imagesf/a2/n02111277_101.JPEG (deflated 1%)\n",
            "updating: content/imagesf/a2/n02111277_94.JPEG (deflated 1%)\n",
            "updating: content/imagesf/a2/n02111277_104.JPEG (deflated 2%)\n",
            "updating: content/imagesf/a2/n02111277_110.JPEG (deflated 2%)\n",
            "updating: content/imagesf/tes.txt (deflated 68%)\n",
            "updating: content/imagesf/tra.txt (deflated 74%)\n",
            "  adding: content/imagesf/.ipynb_checkpoints/ (stored 0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_07115942-11cb-449b-b18b-f267d7ec79d6\", \"imagesf.zip\", 232981)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGS9GPGkWKqd"
      },
      "source": [
        "# try perception\n",
        "\n",
        "class Perceptron:\n",
        "  def __init__(self, n_inputs, activ_func='Maxout', save_fig=True, batch_size=16, learning_rate=0.01, n_iters=1000):\n",
        "      self.__weights = np.array([0.0] * (n_inputs + 1)) # 1 for bias\n",
        "\n",
        "      # self.activation_func = self.activation\n",
        "      self.n_iters = n_iters  # total epochs\n",
        "      self.lr = learning_rate\n",
        "      self.epoch = 1\n",
        "  @property\n",
        "  def train(self, X, y):\n",
        "\n",
        "\n",
        "'''\n",
        "def train(self, datasets):\n",
        "        n_iteration = 0\n",
        "        n_weights = len(self.__weights)\n",
        "        if len(datasets.columns) != n_weights:\n",
        "            raise Exception(\"Wrong inputs of training!\")\n",
        "\n",
        "        x_title, y_title = list(datasets.columns)[:2]\n",
        "        # check_error_results\n",
        "        result = self.check_error(datasets, n_iteration)\n",
        "\n",
        "        while result:\n",
        "            n_iteration += 1\n",
        "            X, target = result\n",
        "            self.update_weights(X, target)\n",
        "\n",
        "            plot_data_and_line(datasets, W=self.weights, iter_time=n_iteration, x_title=x_title, y_title=y_title, save_fig=self.__save_fig)\n",
        "            result = self.check_error(datasets, n_iteration)\n",
        "\n",
        "def check_error(self, datasets, n_iteration):\n",
        "        n_weights = len(self.__weights)\n",
        "        error = 0\n",
        "        result = False\n",
        "        for i in range(datasets.shape[0]):\n",
        "            row = datasets.iloc[i].tolist()\n",
        "            X, target = np.array([1] + row[:-1]), row[-1]  # initial x[0] as 1, for bias, so that w0*x0 = w0 (aka. b)\n",
        "\n",
        "            y = self.activation(np.dot(self.__weights, X)) # W*X = w0*x0 + w1*x1 + ... + w_n*x_n\n",
        "\n",
        "            if target != y:\n",
        "                error += 1\n",
        "                result = X, target\n",
        "        print('Iteration #{}: error = {}'.format(n_iteration, error))\n",
        "        return result\n",
        "'''\n",
        "  def get_loss(self, y_hat, y):\n",
        "    # multi-class cross-entropy\n",
        "    loss += np.dot(y ,np.lop(y_hat))  #??\n",
        "    \n",
        "    print('Epoch #{}: loss = {}'.format(self.epoch, loss))\n",
        "    return loss\n",
        "  \n",
        "  def top_accuracy(self, y_hat, y):\n",
        "    batch_n = y.shape[0] #??\n",
        "    top1 = np.sum(np.equal(np.array([1,2,3,4,5]),np.array([1,3,5,7,9]))) ##\n",
        "    top5 = np.sum(np.isin(element,np.sort(np.argpartition(arr, len(arr) - k)[-k:]))) ##\n",
        "\n",
        "    return top1, top5\n",
        "\n",
        "  def train(self, X, y):\n",
        "    n_weights = len(self.__weights)\n",
        "\n",
        "    if len(X.columns) != n_weights:\n",
        "      raise Exception(\"Wrong inputs of training!\")\n",
        "\n",
        "    n_weights = len(self.__weights)\n",
        "    loss = 0\n",
        "    for i in range(X.shape[0]):\n",
        "      data = np.array([1] + X.iloc[i].tolist())\n",
        "      y_hat = activation(np.dot(self.__weigths, X))\n",
        "    loss = get_loss(y_hat, y)\n",
        "    top1_acc, top5_acc = top_accuracy(y_hat, y)\n",
        "\n",
        "  def weights(self):\n",
        "      return self._weights[:]\n",
        "  \n",
        "  def update_weights(self, X:list, y:int):\n",
        "    self.lr\n",
        "  \n",
        "  def activation(self, wx): # maxout\n",
        "      y_hat = lambda wx: np.exp(wx)/sum(np.exp(wx))\n",
        "      return y_hat = lambda y: np.exp(y)/sum(np.exp(y))\n",
        "\n",
        "# .npy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5IphlKRWfNk"
      },
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "sys.path.append('../../ml')\n",
        "\n",
        "from utils import plot_data_and_line\n",
        "from activation.activation import ActivationFunction\n",
        "from preprocess.iris_preprocess import iris_data_preprocess\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, n_inputs, activ_func='Sign', save_fig=False):\n",
        "        self.__weights = np.array([0.0] * (n_inputs + 1)) # 1 more for bias\n",
        "        self.__save_fig = save_fig\n",
        "        self.__activation = ActivationFunction(activ_func)\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return self.__weights[:]\n",
        "\n",
        "    def update_weights(self, X:list, y:int):\n",
        "        print('>> Updating')\n",
        "\n",
        "        self.__weights += X*y   # w += delta_w --> y * X\n",
        "        print('after update: {}'.format(self.__weights))\n",
        "\n",
        "    def activation(self, y): # sign\n",
        "        return self.__activation.func(y)\n",
        "        # if y > 0:\n",
        "        #     return 1\n",
        "        # else:\n",
        "        #     return -1\n",
        "\n",
        "    def check_error(self, datasets, n_iteration):\n",
        "        n_weights = len(self.__weights)\n",
        "        error = 0\n",
        "        result = False\n",
        "        for i in range(datasets.shape[0]):\n",
        "            row = datasets.iloc[i].tolist()\n",
        "            X, target = np.array([1] + row[:-1]), row[-1]  # initial x[0] as 1, for bias, so that w0*x0 = w0 (aka. b)\n",
        "\n",
        "            y = self.activation(np.dot(self.__weights, X)) # W*X = w0*x0 + w1*x1 + ... + w_n*x_n\n",
        "\n",
        "            if target != y:\n",
        "                error += 1\n",
        "                result = X, target\n",
        "        print('Iteration #{}: error = {}'.format(n_iteration, error))\n",
        "        return result\n",
        "\n",
        "    def train(self, datasets):\n",
        "        n_iteration = 0\n",
        "        n_weights = len(self.__weights)\n",
        "        if len(datasets.columns) != n_weights:\n",
        "            raise Exception(\"Wrong inputs of training!\")\n",
        "\n",
        "        x_title, y_title = list(datasets.columns)[:2]\n",
        "        # check_error_results\n",
        "        result = self.check_error(datasets, n_iteration)\n",
        "\n",
        "        while result:\n",
        "            n_iteration += 1\n",
        "            X, target = result\n",
        "            self.update_weights(X, target)\n",
        "\n",
        "            plot_data_and_line(datasets, W=self.weights, iter_time=n_iteration, x_title=x_title, y_title=y_title, save_fig=self.__save_fig)\n",
        "            result = self.check_error(datasets, n_iteration)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # read data and preprocessing\n",
        "    iris_data = iris_data_preprocess()\n",
        "\n",
        "    # build model\n",
        "    myPerceptron = Perceptron(n_inputs=len(iris_data.columns)-1, save_fig=False)\n",
        "    myPerceptron.train(iris_data)\n",
        "\n",
        "    # plot the result\n",
        "    x_title, y_title = list(iris_data.columns)[:2]\n",
        "    plot_data_and_line(iris_data, myPerceptron.weights, x_title=x_title, y_title=y_title, save_fig=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}